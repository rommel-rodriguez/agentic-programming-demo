{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667548ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import path configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "load_dotenv()\n",
    "logger  = logging.getLogger(__name__)\n",
    "\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    for parent in [start, *start.parents]:\n",
    "        if (parent/\"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "ROOT = _find_project_root(Path.cwd())\n",
    "SRC = ROOT / \"src\"\n",
    "\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46be775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from app.utils.test import print_something\n",
    "from app.adapters.langgraph_agent import LangGraphAgent, MODEL\n",
    "from app.config import configure_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0b5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_tavily import TavilySearch \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "ATTENTION: If you need to look up some information before asking a follow up question, you are allowed to do that! \\\n",
    "you have access to tools the tavily search tool amongs them.\n",
    "IMPORTANT: Make sure to review the tools available to you before making a judgement. \\\n",
    "Also, plan the order of tool calls and reasoning before all else.\n",
    "\"\"\"\n",
    "model = ChatGoogleGenerativeAI(model=MODEL)\n",
    "messages = [HumanMessage(content=\"What is the weather in San Francisco?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = LangGraphAgent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            logger.info(v[\"messages\"])\n",
    "            print(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb4204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 17:13:13,375::DEBUG::httpcore.connection::connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None\n",
      "2026-01-21 17:13:13,457::DEBUG::httpcore.connection::connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f981415ea50>\n",
      "2026-01-21 17:13:13,459::DEBUG::httpcore.connection::start_tls.started ssl_context=<ssl.SSLContext object at 0x7f980712e060> server_hostname='generativelanguage.googleapis.com' timeout=None\n",
      "2026-01-21 17:13:13,505::DEBUG::httpcore.connection::start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f980700bd90>\n",
      "2026-01-21 17:13:13,506::DEBUG::httpcore.http11::send_request_headers.started request=<Request [b'POST']>\n",
      "2026-01-21 17:13:13,507::DEBUG::httpcore.http11::send_request_headers.complete\n",
      "2026-01-21 17:13:13,508::DEBUG::httpcore.http11::send_request_body.started request=<Request [b'POST']>\n",
      "2026-01-21 17:13:13,509::DEBUG::httpcore.http11::send_request_body.complete\n",
      "2026-01-21 17:13:13,509::DEBUG::httpcore.http11::receive_response_headers.started request=<Request [b'POST']>\n",
      "2026-01-21 17:13:15,950::DEBUG::httpcore.http11::receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Wed, 21 Jan 2026 22:13:16 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=2405'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "2026-01-21 17:13:15,951::INFO::httpx::HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 17:13:15,952::DEBUG::httpcore.http11::receive_response_body.started request=<Request [b'POST']>\n",
      "2026-01-21 17:13:15,954::DEBUG::httpcore.http11::receive_response_body.complete\n",
      "2026-01-21 17:13:15,954::DEBUG::httpcore.http11::response_closed.started\n",
      "2026-01-21 17:13:15,954::DEBUG::httpcore.http11::response_closed.complete\n",
      "2026-01-21 17:13:15,958::INFO::app.adapters.langgraph_agent::[AIMessage(content=[{'type': 'text', 'text': 'I apologize, but I cannot provide real-time weather updates as I do not have access to a live weather tool.', 'extras': {'signature': 'CrEEAXLI2nw0lvcPs4+mxJOZyubNEsAoB0IO2AMeY80Tn5SqDuoJ9Q0sbkI4biSFa+3MlJeZDTxai94tf5CEaNmif5p/DBZOwGPSiHyS8AjAGuTW1ZUTJ0X0NIOfUP+/JATAghBnOtBBntYgKkVsCO6Zocjyp84nE0ddnle987154r9KDuwi8hxajNIQsng9GgXtOgopd/mjuU48TgzC/43Su9/yeyVAm+PoVKzzohMv+rfmJbhRvpj5rQxfAavZ9Ow0x7iwOy7t5MCz/QHfPtB4cPAJen9rIbIZKAv1fSP0V9liLloiu98ubPRrbJ77Wgb99ENpt1E3jxbGRbfkWvB5XI9PDXYw0pT9Nlte7kMzDqnpJHN+wmps0ojKRxau5FW/RZ+1YwrXbxh2onTvWGhjh9ZIjpS7GtImqRFha3jaAsnpyGulcBr7vyj1WIGbwbiDORcyrmTTC1ZelzM5paVz01PK0KhWKvg4dUBAQk2f0IYPlBAdToLKguOWnNujVhkdMFi6RKSUMCvLy/Byn8DN7Rmwac7yBLyotzDx9FxzqiTaoJL0wlxpx9dsK0DJNKV20PnKIuymeycIRWfCvqqR80cLaacKVTSv4iU5j2Zq9LSF9G9rDRBkdgFQ/PE755YQ70Yayzzo4wNuhTdE/Pu0jqRn3Fj7Q78MkOM2hwVsFW+kYpOU/rXab7Gn2JFtVYY7469W2M6rl4iCAb1iaN8jYkseypUnm5RUv6/X+AuUuLWg'}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019be29e-721b-7ee2-b226-ea517d335d58-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1529, 'output_tokens': 136, 'total_tokens': 1665, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 112}})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = LangGraphAgent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    abot.query_stream(\"What is the current weather in New York City?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
